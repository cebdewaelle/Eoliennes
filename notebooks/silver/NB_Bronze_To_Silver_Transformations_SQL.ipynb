{"cells":[{"cell_type":"code","source":["%%sql\n","-- Cr√©er une vue temporaire de la table Bronze\n","CREATE OR REPLACE TEMPORARY VIEW bronze_wind_power AS\n","SELECT *\n","FROM LH_Wind_Power_Bronze.dbo.wind_power;\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"3497fbc4-ad67-4f30-b967-75c2194646e0","normalized_state":"finished","queued_time":"2026-02-10T10:38:23.9080012Z","session_start_time":null,"execution_start_time":"2026-02-10T10:38:23.909802Z","execution_finish_time":"2026-02-10T10:38:27.3049565Z","parent_msg_id":"fae254f7-85d9-45dd-bd6d-c488bbf8853d"},"text/plain":"StatementMeta(, 3497fbc4-ad67-4f30-b967-75c2194646e0, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":3,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"e6004be5-b701-4bc9-aba5-b2a4c2739642"},{"cell_type":"code","source":["%%sql\n","-- Nettoyer et enrichir les donn√©es\n","CREATE OR REPLACE TEMPORARY VIEW transformed_wind_power AS\n","SELECT\n","    production_id,\n","    date,\n","    turbine_name,\n","    capacity,\n","    location_name,\n","    latitude,\n","    longitude,\n","    region,\n","    status,\n","    responsible_department,\n","    wind_direction,\n","    \n","    -- üî¢ Arrondi des valeurs num√©riques\n","    ROUND(wind_speed, 2) AS wind_speed,\n","    ROUND(energy_produced, 2) AS energy_produced,\n","    \n","    -- üìÖ Extraction des composants de date\n","    DAY(date) AS day,\n","    MONTH(date) AS month,\n","    QUARTER(date) AS quarter,\n","    YEAR(date) AS year,\n","    \n","    -- üïê Correction du format de time\n","    REGEXP_REPLACE(time, '-', ':') AS time,\n","    \n","    -- ‚è∞ Extraction des composants de temps\n","    CAST(SUBSTRING(time, 1, 2) AS INT) AS hour_of_day,\n","    CAST(SUBSTRING(time, 4, 2) AS INT) AS minute_of_hour,\n","    CAST(SUBSTRING(time, 7, 2) AS INT) AS second_of_minute,\n","    \n","    -- üåÖ Calcul de la p√©riode de la journ√©e\n","    CASE\n","        WHEN CAST(SUBSTRING(time, 1, 2) AS INT) BETWEEN 5 AND 11 THEN 'Morning'\n","        WHEN CAST(SUBSTRING(time, 1, 2) AS INT) BETWEEN 12 AND 16 THEN 'Afternoon'\n","        WHEN CAST(SUBSTRING(time, 1, 2) AS INT) BETWEEN 17 AND 20 THEN 'Evening'\n","        ELSE 'Night'\n","    END AS time_period\n","    \n","FROM bronze_wind_power;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"3497fbc4-ad67-4f30-b967-75c2194646e0","normalized_state":"finished","queued_time":"2026-02-10T10:38:43.2804909Z","session_start_time":null,"execution_start_time":"2026-02-10T10:38:43.2822916Z","execution_finish_time":"2026-02-10T10:38:44.739654Z","parent_msg_id":"3639cf3d-2659-43e1-b71a-fed80910678b"},"text/plain":"StatementMeta(, 3497fbc4-ad67-4f30-b967-75c2194646e0, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"error","ename":"Error","evalue":"[REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `LH_Wind_Power_Bronze`.`dbo`.","traceback":["[REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `LH_Wind_Power_Bronze`.`dbo`.","org.apache.spark.sql.errors.QueryCompilationErrors$.requiresSinglePartNamespaceError(QueryCompilationErrors.scala:1346)","org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog$TableIdentifierHelper.asTableIdentifier(V2SessionCatalog.scala:258)","org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:81)","org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.loadTable(DelegatingCatalogExtension.java:73)","org.apache.spark.sql.delta.catalog.DeltaCatalog.super$loadTable(DeltaCatalog.scala:222)","org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$loadTable$1(DeltaCatalog.scala:222)","org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)","org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)","org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:66)","org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:221)","org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)","org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$getLoadedTable$4(Analyzer.scala:1495)","org.apache.spark.microsoft.onesecurity.OneSecurityTelemetry$.executeAndLogMetricMs(OneSecurityTelemetry.scala:321)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$getLoadedTable$2(Analyzer.scala:1495)","org.apache.spark.microsoft.onesecurity.OneSecurityTelemetry$.executeAndLogMetricMs(OneSecurityTelemetry.scala:321)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getLoadedTable(Analyzer.scala:1492)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1419)","scala.Option.orElse(Option.scala:447)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1411)","scala.Option.orElse(Option.scala:447)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1391)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1238)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1202)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:140)","org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:140)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:334)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:136)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:132)","org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:36)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:137)","org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)","org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)","org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:137)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:334)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:136)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:132)","org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:36)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:137)","org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)","org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)","org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:137)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:334)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:136)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:132)","org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:36)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1202)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1161)","org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)","scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)","scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)","scala.collection.immutable.List.foldLeft(List.scala:91)","org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)","org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)","scala.collection.immutable.List.foreach(List.scala:431)","org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)","org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:248)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveViews$2(Analyzer.scala:1182)","org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:163)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveViews$1(Analyzer.scala:1182)","org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withAnalysisContext(Analyzer.scala:189)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1174)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1190)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.$anonfun$applyOrElse$76(Analyzer.scala:1238)","scala.Option.map(Option.scala:230)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1238)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1202)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:140)","org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:140)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:334)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:136)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:132)","org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:36)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:137)","org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)","org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)","org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:137)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:334)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:136)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:132)","org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:36)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:137)","scala.collection.immutable.List.map(List.scala:293)","org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:137)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:334)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:136)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:132)","org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:36)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1202)","org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1161)","org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)","scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)","scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)","scala.collection.immutable.List.foldLeft(List.scala:91)","org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)","org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)","scala.collection.immutable.List.foreach(List.scala:431)","org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)","org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:248)","org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:244)","org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:195)","org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:244)","org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:210)","org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)","org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:94)","org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)","org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:231)","org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:341)","org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:230)","org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:195)","org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)","org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:466)","org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:837)","org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:466)","org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:955)","org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:465)","org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:195)","org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:184)","org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:117)","org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)","org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:955)","org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:98)","org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:746)","org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:955)","org.apache.spark.sql.SparkSession.sql(SparkSession.scala:737)","org.apache.spark.sql.SparkSession.sql(SparkSession.scala:768)","org.apache.spark.sql.SparkSession.sql(SparkSession.scala:799)","org.apache.livy.repl.SQLInterpreter.execute(SQLInterpreter.scala:164)","org.apache.livy.repl.Session.$anonfun$executeCode$1(Session.scala:924)","scala.Option.map(Option.scala:230)","org.apache.livy.repl.Session.executeCode(Session.scala:921)","org.apache.livy.repl.Session.$anonfun$execute$17(Session.scala:612)","org.apache.livy.repl.Session.withRealtimeOutputSupport(Session.scala:1175)","org.apache.livy.repl.Session.$anonfun$execute$3(Session.scala:612)","scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)","scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","scala.util.Success.$anonfun$map$1(Try.scala:255)","scala.util.Success.map(Try.scala:213)","scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","java.base/java.lang.Thread.run(Thread.java:829)"]}],"execution_count":4,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"}},"id":"d0a7403b-243f-4770-b406-bceadac6aef0"},{"cell_type":"code","source":["%%sql\n","-- Supprimer l'ancienne table Silver si elle existe\n","DROP TABLE IF EXISTS LH_Wind_Power_Silver.dbo.wind_power;"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"}},"id":"e164134a-e19d-43dc-9338-e12f489334ca"},{"cell_type":"code","source":["%%sql\n","-- Cr√©er la nouvelle table Silver avec les donn√©es transform√©es\n","CREATE TABLE LH_Wind_Power_Silver.dbo.wind_power\n","USING delta\n","AS\n","SELECT * FROM transformed_wind_power;"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"}},"id":"c50d683e-4754-4f1c-84de-ce4faee3916b"},{"cell_type":"code","source":["%%sql\n","-- V√©rifier que la table a √©t√© cr√©√©e avec succ√®s\n","SELECT \n","    COUNT(*) as total_rows,\n","    MIN(date) as min_date,\n","    MAX(date) as max_date,\n","    COUNT(DISTINCT turbine_name) as turbine_count\n","FROM LH_Wind_Power_Silver.dbo.wind_power;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"3497fbc4-ad67-4f30-b967-75c2194646e0","normalized_state":"finished","queued_time":"2026-02-10T10:37:28.4138253Z","session_start_time":null,"execution_start_time":"2026-02-10T10:37:28.4156458Z","execution_finish_time":"2026-02-10T10:37:51.0473936Z","parent_msg_id":"1fe24e79-819e-4fc9-824d-bdb0cfadbaff"},"text/plain":"StatementMeta(, 3497fbc4-ad67-4f30-b967-75c2194646e0, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":2,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"total_rows","type":"long","nullable":false,"metadata":{}},{"name":"min_date","type":"timestamp","nullable":true,"metadata":{}},{"name":"max_date","type":"timestamp","nullable":true,"metadata":{}},{"name":"turbine_count","type":"long","nullable":false,"metadata":{}}]},"data":[["864","2024-06-15T00:00:00Z","2024-06-16T00:00:00Z","3"]]},"text/plain":"<Spark SQL result set with 1 rows and 4 fields>"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"7609f237-288a-4e96-aab2-ae099a5d0f21"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"fr"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"4c66c7f4-ce54-489f-9738-d048af5894d6"},{"id":"724e2441-3064-4829-9858-9434190c2da8"}],"default_lakehouse":"4c66c7f4-ce54-489f-9738-d048af5894d6","default_lakehouse_name":"LH_Wind_Power_Silver","default_lakehouse_workspace_id":"8f015743-6f02-47e8-8086-1af3ad394858"}}},"nbformat":4,"nbformat_minor":5}